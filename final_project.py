# -*- coding: utf-8 -*-
"""Final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvNZB0fCGtsiChvt5RVCQ9aHFNqMMJQO
"""

from google.colab import drive
drive.mount("/content/drive")

"""### Cifar-10 Dataset"""

import matplotlib.pyplot as plt
import numpy as np
from collections import defaultdict
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import Dataset
import torch
from torch.utils.data import DataLoader, Subset
import torch.nn as nn
import torch.nn.functional as F

import sys
!{sys.executable} -m pip install flwr
!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Load the CIFAR-10 dataset to get its targets for partitioning
cifar10_train_dataset = datasets.CIFAR10(
  root='./data',
  train=True,
  download=True,
  transform=transforms.ToTensor()
)

# Define the partition_data function as provided in Appendix A
def partition_data(dataset, num_clients, alpha, seed=42):
  np.random.seed(seed)
  num_classes = 10
  min_size = 0
  train_labels = np.array(dataset.targets)

  while min_size < 10:
    idx_batch = [[] for _ in range(num_clients)]
    for k in range(num_classes):
      idx_k = np.where(train_labels == k)[0] # Ensure idx_k is a 1D array of indices
      np.random.shuffle(idx_k)
      proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
      proportions = np.array([p / proportions.sum() for p in proportions])
      proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]

      # Distribute indices for class k among clients
      split_indices = np.split(idx_k, proportions)
      for i, client_indices in enumerate(split_indices):
        idx_batch[i].extend(client_indices.tolist())
    min_size = min([len(idx_j) for idx_j in idx_batch])
    if min_size < 10:
      print("Re-partitioning due to client with less than 10 samples.")

  return idx_batch

num_clients = 10
alpha = 0.1
client_data_indices = partition_data(cifar10_train_dataset, num_clients, alpha)

print(f"Partitioned CIFAR-10 training data for {num_clients} clients with alpha={alpha}:")
for i, indices in enumerate(client_data_indices):
  print(f"Client {i+1}: {len(indices)} samples")

"""# Central Learning

### IndexedSUbset + DataLoader
"""

class IndexedSubset(Dataset):
  def __init__(self, dataset, indices):
    self.dataset = dataset
    self.indices = indices

  def __len__(self):
    return len(self.indices)

  def __getitem__(self, i):
    real_idx = self.indices[i]  # index in raw training dataset
    x, y = self.dataset[real_idx]
    return x, y, real_idx


def get_client_loader(dataset, indices, batch_size=64, shuffle=True):
  subset = IndexedSubset(dataset, indices)
  return DataLoader(subset, batch_size=batch_size, shuffle=shuffle)

test_transform = transforms.Compose([
  transforms.ToTensor()
])

cifar10_test_dataset = datasets.CIFAR10(
  root='./data',
  train=False,
  download=True,
  transform=test_transform
)
testloader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False)

"""### Simple CNN"""

class SimpleCIFAR10CNN(nn.Module):
  def __init__(self, num_classes=10):
    super().__init__()
    #(B, 3, 32, 32)
    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
    self.pool  = nn.MaxPool2d(2, 2)
    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
    self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
    self.fc1 = nn.Linear(128 * 8 * 8, 256)
    self.fc2 = nn.Linear(256, num_classes)

  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.relu(self.conv2(x))
    x = self.pool(x)
    x = F.relu(self.conv3(x))
    x = F.relu(self.conv4(x))
    x = self.pool(x)
    x = torch.flatten(x,1)
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x

"""### Evaluate function"""

def evaluate(model, dataloader, device):
  model.eval()
  correct, total = 0, 0
  with torch.no_grad():
    for inputs, targets in dataloader:
      inputs, targets = inputs.to(device), targets.to(device)
      outputs = model(inputs)
      _, predicted = outputs.max(1)
      total += targets.size(0)
      correct += predicted.eq(targets).sum().item()
  return correct / total

"""### warm-up epoch"""

def train_one_epoch_warmup(model, dataloader, optimizer, criterion, device,
                           signal_history=None, num_classes=10):
  model.train()
  running_loss = 0.0
  correct = 0
  total = 0

  for batch_idx, (inputs, targets, idxs) in enumerate(dataloader):
    inputs, targets = inputs.to(device), targets.to(device)

    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    if signal_history is not None:
      with torch.no_grad():
        probs = F.softmax(outputs, dim=1)
        entropy = -(probs * (probs + 1e-12).log()).sum(dim=1)

        one_hot = F.one_hot(targets, num_classes=num_classes).float()
        el2n = (probs - one_hot).pow(2).sum(dim=1).sqrt()

    loss.backward()

    if signal_history is not None:
      grad_norm = 0.0
      for p in model.fc2.parameters():   # only look the last layer
        if p.grad is not None:
          grad_norm += p.grad.detach().pow(2).sum()
      grad_norm = grad_norm.sqrt().item()

      # Store signal: once for each sample
      idxs_np = idxs.cpu().numpy()
      ent_np = entropy.cpu().numpy()
      el2n_np = el2n.cpu().numpy()
      for i_idx, ent, e2 in zip(idxs_np, ent_np, el2n_np):
        signal_history["entropy"][i_idx].append(ent)
        signal_history["el2n"][i_idx].append(e2)
        signal_history["grad_norm"][i_idx].append(grad_norm)

    optimizer.step()
    running_loss += loss.item() * inputs.size(0)
    _, predicted = outputs.max(1)
    total += targets.size(0)
    correct += predicted.eq(targets).sum().item()

    if (batch_idx + 1) % 100 == 0:
      print(f"Batch {batch_idx+1}/{len(dataloader)}, "
        f"Loss: {loss.item():.4f}")

  epoch_loss = running_loss / total
  epoch_acc = correct / total
  return epoch_loss, epoch_acc

"""# Federated Learning"""

client_loaders = {
  cid: get_client_loader(cifar10_train_dataset, idxs, batch_size=batch_size, shuffle=True)
  for cid, idxs in enumerate(client_data_indices)
}

num_clients = len(client_loaders)
print("Total clients:", num_clients)

def get_model_weights(model):
  # return a copy
  return {k: v.cpu().clone() for k, v in model.state_dict().items()}

def set_model_weights(model, weights):
  model.load_state_dict(weights)

def fedavg(client_weight_list):
  """
  client_weight_list: list of (weights_dict, n_samples)
  """
  total_samples = sum(n for _, n in client_weight_list)
  agg_weights = {}
  for k in client_weight_list[0][0].keys():
    agg_weights[k] = sum(w[k] * (n / total_samples) for w, n in client_weight_list)
  return agg_weights

def train_one_epoch_local(model, dataloader, optimizer, criterion, device):
  model.train()
  running_loss, correct, total = 0.0, 0, 0

  for batch_idx, (inputs, targets, idxs) in enumerate(dataloader):
    inputs, targets = inputs.to(device), targets.to(device)

    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()

    running_loss += loss.item() * inputs.size(0)
    _, predicted = outputs.max(1)
    total += targets.size(0)
    correct += predicted.eq(targets).sum().item()

  epoch_loss = running_loss / total
  epoch_acc = correct / total
  return epoch_loss, epoch_acc

def local_warmup_and_select(
  global_weights,
  client_loader,
  keep_ratio=0.5,
  warmup_epochs=2,
  local_epochs=1,
  num_classes=10,
  device="cuda" if torch.cuda.is_available() else "cpu",
  use_grad=True,
  use_entropy=True,
  use_el2n=True,
  use_stability=True,
):
  model = SimpleCIFAR10CNN(num_classes=num_classes).to(device)
  set_model_weights(model, global_weights)
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

  signal_history_local = {
    "entropy": defaultdict(list),
    "el2n": defaultdict(list),
    "grad_norm": defaultdict(list),
  }
  for epoch in range(warmup_epochs):
    train_loss, train_acc = train_one_epoch_warmup(
      model,
      client_loader,
      optimizer,
      criterion,
      device,
      epoch=epoch,
      signal_history=signal_history_local,
      num_classes=num_classes,
    )

  avg_entropy, avg_el2n, avg_grad, stability = {}, {}, {}, {}
  candidate_indices = list(signal_history_local["entropy"].keys())

  for idx in candidate_indices:
    e_list = signal_history_local["entropy"][idx]
    el_list = signal_history_local["el2n"][idx]
    g_list = signal_history_local["grad_norm"][idx]

    avg_entropy[idx] = float(np.mean(e_list))
    avg_el2n[idx] = float(np.mean(el_list))
    avg_grad[idx] = float(np.mean(g_list))
    stability[idx] = float(np.var(e_list))

  # hybrid score
  alpha_g = 0.5
  beta_e  = 0.3
  gamma_el = 0.2
  delta_s = 0.1

  scores = []
  for idx in candidate_indices:
    score = 0.0
    if use_grad:
      score += alpha_g * avg_grad[idx]
    if use_entropy:
      score += beta_e * avg_entropy[idx]
    if use_el2n:
      score += gamma_el * avg_el2n[idx]
    if use_stability:
      score -= delta_s * stability[idx]

    scores.append((score, idx))

  scores.sort(reverse=True, key=lambda x: x[0])
  k = int(len(scores) * keep_ratio)
  selected_indices = [idx for _, idx in scores[:k]]

  subset_loader = get_client_loader(
    cifar10_train_dataset,
    selected_indices,
    batch_size=batch_size,
    shuffle=True,
  )

  # train local_epochs on subset
  for epoch in range(local_epochs):
    train_loss, train_acc = train_one_epoch_local(
      model,
      subset_loader,
      optimizer,
      criterion,
      device,
    )

  local_weights = get_model_weights(model)
  num_selected = len(selected_indices)
  return local_weights, num_selected

"""# Baseline function"""

def local_train_full(
    global_weights,
    client_loader,
    local_epochs=1,
    num_classes=10,
    device="cuda" if torch.cuda.is_available() else "cpu",
):
  model = SimpleCIFAR10CNN(num_classes=num_classes).to(device)
  set_model_weights(model, global_weights)
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

  n_samples = len(client_loader.dataset)

  for epoch in range(local_epochs):
    train_loss, train_acc = train_one_epoch_local(
      model,
      client_loader,
      optimizer,
      criterion,
      device,
    )
  local_weights = get_model_weights(model)
  return local_weights, n_samples

"""## Random Baseline"""

def local_random_subset(
  global_weights,
  client_loader,
  keep_ratio=0.5,
  local_epochs=1,
  num_classes=10,
  device="cuda" if torch.cuda.is_available() else "cpu",
):
  model = SimpleCIFAR10CNN(num_classes=num_classes).to(device)
  set_model_weights(model, global_weights)
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

  # actual indexes from the IndexedSubset
  all_indices = client_loader.dataset.indices
  all_indices = np.array(all_indices)
  k = int(len(all_indices) * keep_ratio)
  selected_indices = np.random.choice(all_indices, size=k, replace=False)

  subset_loader = get_client_loader(
    cifar10_train_dataset,
    selected_indices,
    batch_size=batch_size,
    shuffle=True,
  )

  for epoch in range(local_epochs):
    train_loss, train_acc = train_one_epoch_local(
      model,
      subset_loader,
      optimizer,
      criterion,
      device,
    )

  local_weights = get_model_weights(model)
  return local_weights, len(selected_indices)

"""# Federated experiment"""

def run_federated_experiment(
  num_rounds=10,
  mode="hybrid",  # "full" (no selection), "random", "hybrid"
  warmup_epochs=1,
  keep_ratio=0.5,
  local_epochs=1,
  use_grad=True,
  use_entropy=True,
  use_el2n=True,
  use_stability=True,
  tag="",
  device=device,
):
  global_model = SimpleCIFAR10CNN(num_classes=10).to(device)
  test_acc_history = []

  for rnd in range(num_rounds):
    print(f"[{tag}] Round {rnd+1}/{num_rounds}")
    global_weights = get_model_weights(global_model)
    client_weight_list = []

    for cid in range(num_clients):
      client_loader = client_loaders[cid]

      if mode == "full":
        local_w, n_used = local_train_full(
          global_weights,
          client_loader,
          local_epochs=local_epochs,
          num_classes=10,
          device=device,
        )
      elif mode == "random":
        local_w, n_used = local_random_subset(
          global_weights,
          client_loader,
          keep_ratio=keep_ratio,
          local_epochs=local_epochs,
          num_classes=10,
          device=device,
        )
      elif mode == "hybrid":
        # warm-up + hybrid scoring
        local_w, n_used = local_warmup_and_select(
          global_weights,
          client_loader,
          keep_ratio=keep_ratio,
          warmup_epochs=warmup_epochs,
          local_epochs=local_epochs,
          num_classes=10,
          device=device,
          use_grad=use_grad,
          use_entropy=use_entropy,
          use_el2n=use_el2n,
          use_stability=use_stability,
        )
      else:
        raise ValueError(f"Unknown mode: {mode}")

      client_weight_list.append((local_w, n_used))
      print(f"Client {cid}: used {n_used} samples")

    new_global_weights = fedavg(client_weight_list)
    set_model_weights(global_model, new_global_weights)

    test_acc = evaluate(global_model, testloader, device)
    test_acc_history.append(test_acc)
    print(f">>Test Acc after round {rnd+1}: {test_acc:.4f}")

  return test_acc_history

results_by_warmup = {}
print("\n Baseline: full data, no selection")
results_by_warmup["full_baseline"] = run_federated_experiment(
  num_rounds=10,
  mode="full",
  local_epochs=1,
  tag="full_baseline",
)
print("\n Random subset, no warm-up (warmup=0)")
results_by_warmup["random_wu0"] = run_federated_experiment(
  num_rounds=10,
  mode="random",
  warmup_epochs=0,
  keep_ratio=0.5,
  local_epochs=1,
  tag="random_wu0",
)
for wu in [1, 2, 4]:
  print(f"\n Hybrid selection, warmup_epochs={wu}")
  key = f"hybrid_wu{wu}"
  results_by_warmup[key] = run_federated_experiment(
    num_rounds=10,
    mode="hybrid",
    warmup_epochs=wu,
    keep_ratio=0.5,
    local_epochs=1,
    use_grad=True,
    use_entropy=True,
    use_el2n=True,
    use_stability=True,
    tag=key,
  )

import matplotlib.pyplot as plt

num_rounds = len(next(iter(results_by_warmup.values())))
rounds = list(range(1, num_rounds + 1))

plt.figure(figsize=(6, 4))

for name, acc_list in results_by_warmup.items():
  # In case dismatch length
  r = list(range(1, len(acc_list) + 1))
  plt.plot(r, acc_list, marker='o', label=name)

plt.xlabel("Federated Round")
plt.ylabel("Test Accuracy")
plt.title("Test Accuracy vs Federated Rounds (Different Warm-up Settings)")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""

1. Random sub-sampling without any warm-up leads to very poor performance, showing that simply dropping 50% of data is not sufficient
2. Even with a single warm-up epoch which is hybrid_wu1, our method already outperforms the full-data FedAvg baseline by a large margin, which shows that we can select an effective subset very early in training.
3. Increasing the warm-up length to 2 or 4 epochs further accelerates convergence and improves final accuracy, but the gain from 2 to 4 epochs is much smaller than from 0 to 1 or 1 to 2, indicating diminishing returns of longer warm-up.
"""

plt.figure(figsize=(6, 4))

plt.plot(rounds, results_by_warmup["full_baseline"], marker='o', label="full_baseline")
plt.plot(rounds, results_by_warmup["hybrid_wu2"], marker='x', label="hybrid_wu2")

plt.xlabel("Federated Round")
plt.ylabel("Test Accuracy")
plt.title("Baseline vs Hybrid (warmup=2)")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""With a 2-epoch local warm-up, our hybrid data selection consistently outperforms the full-data FedAvg baseline at every communication round, achieving about +20 percentage points higher test accuracy by round 10.

# Single Ablation
"""

results_ablation = {}

config_list = [
  ("hybrid_all", dict(use_grad=True, use_entropy=True, use_el2n=True, use_stability=True)),
  ("no_stability", dict(use_grad=True, use_entropy=True, use_el2n=True, use_stability=False)),
  ("grad_only", dict(use_grad=True, use_entropy=False, use_el2n=False, use_stability=False)),
  ("entropy_only", dict(use_grad=False, use_entropy=True, use_el2n=False, use_stability=False)),
  ("el2n_only", dict(use_grad=False, use_entropy=False, use_el2n=True, use_stability=False)),
]

for name, flags in config_list:
  print(f"\n Ablation: {name}")
  results_ablation[name] = run_federated_experiment(
    num_rounds=10,
    mode="hybrid",
    warmup_epochs=2,
    keep_ratio=0.5,
    local_epochs=1,
    tag=name,
    **flags,
  )

names = list(results_ablation.keys())
final_accs = [acc_list[-1] for acc_list in results_ablation.values()]

x = np.arange(len(names))

plt.figure(figsize=(6, 4))
plt.bar(x, final_accs)
plt.xticks(x, names, rotation=30, ha='right')
plt.ylabel("Final Test Accuracy")
plt.title("Final Test Accuracy by Signal Configuration")
plt.tight_layout()
plt.show()

base = results_ablation["hybrid_all"]
rounds = list(range(1, len(base) + 1))

plt.figure(figsize=(6, 4))

for name, acc_list in results_ablation.items():
  if name == "hybrid_all":
    continue
  diffs = [a - b for a, b in zip(acc_list, base)]
  plt.plot(rounds, diffs, marker='o', label=name)

plt.axhline(0.0, linestyle='--')
plt.xlabel("Federated Round")
plt.ylabel("Accuracy Difference vs hybrid_all")
plt.title("Accuracy Gap to Full Hybrid Configuration")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""In our ablation study, all signal configurations achieve very similar performance, with final test accuracy in the range of 55-58%. The accuracy gap to the full hybrid configuration is generally within ±2% after a few communication rounds.

This suggests that each individual signal already captures a reasonably good notion of sample importance in our federated CIFAR-10 dataset. The stability term provides only marginal benefit and can even slightly hurt performance, likely because the warm-up phase is short and the variance estimates are noisy. Overall, our method is robust to the choice of signal, and the hybrid combination does not drastically outperform simpler variants under this training budget.

# Warm-up and subset selection are only performed in the first few rounds; no further selection is done afterwards.
"""

def local_warmup_and_select_return_indices(
    global_weights,
    client_loader,
    keep_ratio=0.5,
    warmup_epochs=2,
    local_epochs=1,
    num_classes=10,
    device="cuda" if torch.cuda.is_available() else "cpu",
    use_grad=True,
    use_entropy=True,
    use_el2n=True,
    use_stability=True,
):
  # return indices
  model = SimpleCIFAR10CNN(num_classes=num_classes).to(device)
  set_model_weights(model, global_weights)
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

  from collections import defaultdict
  signal_history_local = {
    "entropy": defaultdict(list),
    "el2n": defaultdict(list),
    "grad_norm": defaultdict(list),
  }

  for epoch in range(warmup_epochs):
    train_loss, train_acc = train_one_epoch_warmup(
      model,
      client_loader,
      optimizer,
      criterion,
      device,
      epoch=epoch,
      signal_history=signal_history_local,
      num_classes=num_classes,
    )

  import numpy as np
  avg_entropy, avg_el2n, avg_grad, stability = {}, {}, {}, {}
  candidate_indices = list(signal_history_local["entropy"].keys())

  for idx in candidate_indices:
    e_list = signal_history_local["entropy"][idx]
    el_list = signal_history_local["el2n"][idx]
    g_list = signal_history_local["grad_norm"][idx]

    avg_entropy[idx] = float(np.mean(e_list))
    avg_el2n[idx] = float(np.mean(el_list))
    avg_grad[idx] = float(np.mean(g_list))
    stability[idx] = float(np.var(e_list))

  alpha_g = 0.5
  beta_e = 0.3
  gamma_el = 0.2
  delta_s = 0.1

  scores = []
  for idx in candidate_indices:
    score = 0.0
    if use_grad:
      score += alpha_g * avg_grad[idx]
    if use_entropy:
      score += beta_e * avg_entropy[idx]
    if use_el2n:
      score += gamma_el * avg_el2n[idx]
    if use_stability:
      score -= delta_s * stability[idx]
    scores.append((score, idx))

  scores.sort(reverse=True, key=lambda x: x[0])
  k = int(len(scores) * keep_ratio)
  selected_indices = [idx for _, idx in scores[:k]]

  subset_loader = get_client_loader(
    cifar10_train_dataset,
    selected_indices,
    batch_size=batch_size,
    shuffle=True,
  )

  for epoch in range(local_epochs):
    train_loss, train_acc = train_one_epoch_local(
      model,
      subset_loader,
      optimizer,
      criterion,
      device,
    )

  local_weights = get_model_weights(model)
  return local_weights, selected_indices

def run_federated_experiment_one_time_selection(
    num_rounds=10,
    warmup_rounds=1,       # warm-up for first few rounds
    warmup_epochs=2,
    keep_ratio=0.5,
    local_epochs=1,
    tag="one_time",
    device=device,
):
  global_model = SimpleCIFAR10CNN(num_classes=10).to(device)
  test_acc_history = []

  selected_indices_per_client = {cid: None for cid in range(num_clients)}

  for rnd in range(num_rounds):
    print(f"[{tag}] Round {rnd+1}/{num_rounds}")
    global_weights = get_model_weights(global_model)
    client_weight_list = []

    for cid in range(num_clients):
      client_loader = client_loaders[cid]

      if (rnd < warmup_rounds) or (selected_indices_per_client[cid] is None):
        # warm-up + select subset if not
        local_w, selected_indices = local_warmup_and_select_return_indices(
          global_weights,
          client_loader,
          keep_ratio=keep_ratio,
          warmup_epochs=warmup_epochs,
          local_epochs=local_epochs,
          num_classes=10,
          device=device,
          use_grad=True,
          use_entropy=True,
          use_el2n=True,
          use_stability=True,
        )
        selected_indices_per_client[cid] = selected_indices
        n_used = len(selected_indices)
      else:
        # no warm up, using same subset
        subset_loader = get_client_loader(
          cifar10_train_dataset,
          selected_indices_per_client[cid],
          batch_size=batch_size,
          shuffle=True,
        )
        local_w, n_used = local_train_full(
          global_weights,
          subset_loader,
          local_epochs=local_epochs,
          num_classes=10,
          device=device,
        )

      client_weight_list.append((local_w, n_used))
      print(f"Client {cid}: used {n_used} samples")

    new_global_weights = fedavg(client_weight_list)
    set_model_weights(global_model, new_global_weights)

    test_acc = evaluate(global_model, testloader, device)
    test_acc_history.append(test_acc)
    print(f">> Test Acc after round {rnd+1}: {test_acc:.4f}")

  return test_acc_history

results_once = {}

# warm up every round
results_once["hybrid_every_round"] = results_by_warmup["hybrid_wu2"]

# Only warm up for first round
results_once["hybrid_once_round1"] = run_federated_experiment_one_time_selection(
  num_rounds=10,
  warmup_rounds=1,
  warmup_epochs=2,
  keep_ratio=0.5,
  local_epochs=1,
  tag="once_r1",
)

# Only warm-up at first two rounds
results_once["hybrid_once_round2"] = run_federated_experiment_one_time_selection(
  num_rounds=10,
  warmup_rounds=2,
  warmup_epochs=2,
  keep_ratio=0.5,
  local_epochs=1,
  tag="once_r2",
)

plt.figure(figsize=(6,4))
for name, acc_list in results_once.items():
  r = list(range(1, len(acc_list)+1))
  plt.plot(r, acc_list, marker='o', label=name)

plt.xlabel("Federated Round")
plt.ylabel("Test Accuracy")
plt.title("Every-round vs One-time Warm-up Selection")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""As shown in the figure, updating the selected subset at every round achieves the best performance, while one-time selection at round 1 or rounds 2 leads to lower final accuracy. This indicates that although we can obtain a reasonably good subset at the very early stage, continuously refreshing the subset still provides noticeable improvement.

# Change keep_ratio to see the advantages of selection when saving more data
"""

keep_ratios = [0.25, 0.5, 0.75]
results_keep = {}

for kr in keep_ratios:
  print(f"\n keep_ratio = {kr}, RANDOM subset")
  results_keep[f"random_kr{kr}"] = run_federated_experiment(
    num_rounds=10,
    mode="random",
    warmup_epochs=0,
    keep_ratio=kr,
    local_epochs=1,
    tag=f"random_kr{kr}",
  )

  print(f"\n keep_ratio = {kr}, HYBRID with warmup=2")
  results_keep[f"hybrid_kr{kr}"] = run_federated_experiment(
    num_rounds=10,
    mode="hybrid",
    warmup_epochs=2,
    keep_ratio=kr,
    local_epochs=1,
    use_grad=True,
    use_entropy=True,
    use_el2n=True,
    use_stability=True,
    tag=f"hybrid_kr{kr}",
  )

import numpy as np
import matplotlib.pyplot as plt

final_random = []
final_hybrid = []

for kr in keep_ratios:
  final_random.append(results_keep[f"random_kr{kr}"][-1])
  final_hybrid.append(results_keep[f"hybrid_kr{kr}"][-1])

x = np.arange(len(keep_ratios))

plt.figure(figsize=(6,4))
plt.plot(keep_ratios, final_random, marker='o', label='random')
plt.plot(keep_ratios, final_hybrid, marker='x', label='hybrid')
plt.xlabel("keep_ratio (fraction of local data)")
plt.ylabel("Final Test Accuracy")
plt.title("Effect of keep_ratio: Random vs Hybrid Selection")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""With hybrid selection, keeping only 25% of the local data already outperforms a 75% random subset by a large margin, showing that data quality matters much more than quantity.

But hybrid selection exhibits diminishing returns as keep_ratio increases: the model already achieves strong performance with only 25% of the data, and adding more samples brings incremental but smaller gains

# Different $α$
"""

def build_client_loaders_for_alpha(alpha, num_clients=10, batch_size=64):
  client_data_indices = partition_data(
    cifar10_train_dataset,
    num_clients=num_clients,
    alpha=alpha,
  )
  client_loaders = {
    cid: get_client_loader(cifar10_train_dataset, idxs, batch_size=batch_size, shuffle=True)
    for cid, idxs in enumerate(client_data_indices)
  }
  return client_data_indices, client_loaders

alphas = [0.1, 0.5, 1.0]
results_by_alpha = {}

for alpha in alphas:
  print(f"\n Dirichlet alpha = {alpha}")
  client_data_indices, client_loaders = build_client_loaders_for_alpha(alpha, num_clients=num_clients, batch_size=batch_size)

  # baseline
  print("\n full_baseline")
  results_by_alpha[(alpha, "baseline")] = run_federated_experiment(
    num_rounds=10,
    mode="full",
    local_epochs=1,
    tag=f"baseline_a{alpha}",
  )

  # hybrid with warmup 2
  print("\n hybrid_wu2")
  results_by_alpha[(alpha, "hybrid_wu2")] = run_federated_experiment(
    num_rounds=10,
    mode="hybrid",
    warmup_epochs=2,
    keep_ratio=0.5,
    local_epochs=1,
    use_grad=True,
    use_entropy=True,
    use_el2n=True,
    use_stability=True,
    tag=f"hybrid_wu2_a{alpha}",
  )

baseline_finals = []
hybrid_finals = []

for alpha in alphas:
  base_final = results_by_alpha[(alpha, "baseline")][-1]
  hybr_final = results_by_alpha[(alpha, "hybrid_wu2")][-1]
  baseline_finals.append(base_final)
  hybrid_finals.append(hybr_final)

plt.figure(figsize=(6,4))
plt.plot(alphas, baseline_finals, marker='o', label='baseline')
plt.plot(alphas, hybrid_finals,   marker='x', label='hybrid_wu2')

plt.xlabel("Dirichlet alpha")
plt.ylabel("Accuracy")
plt.title("Baseline vs Hybrid")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""We vary the Dirichlet $\alpha$ controlling data heterogeneity and observe that both the baseline and our hybrid method perform better as $\alpha$ increases. Our method consistently outperforms the baseline by about 17-21 percentage across all $\alpha$ values, showing that the benefit of warm-up based data selection is robust to different levels of client heterogeneity."""